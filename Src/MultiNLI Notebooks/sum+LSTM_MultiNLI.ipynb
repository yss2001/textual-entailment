{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summation & LSTM Model - MultiNLI Dataset\n",
    "\n",
    "The following notebook contains the implementation of the baseline summation model, and the standard LSTM model for the MultiNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/aakashj2412/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import re\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.regularizers import L2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, Input, LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that cleans the input data and enumerates the labels\n",
    "\n",
    "def extract(s):\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "labels = {'entailment': 0, 'contradiction': 1, 'neutral': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that reads data and parses data from file\n",
    "\n",
    "def readFileData(filePath, t):\n",
    "    with open(filePath, 'r') as f:\n",
    "        inputRows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    \n",
    "    if t == 't':\n",
    "        inputPremises = [extract(row[1]) for row in inputRows if row[0] in labels]\n",
    "        inputHypotheses = [extract(row[2]) for row in inputRows if row[0] in labels]\n",
    "        inputLabels = [labels[row[0]] for row in inputRows if row[0] in labels]\n",
    "        f.close()\n",
    "\n",
    "        return [inputPremises, inputHypotheses, inputLabels]\n",
    "\n",
    "    else:\n",
    "        test, val = train_test_split(inputRows, test_size=0.2, random_state=42)\n",
    "        \n",
    "        testPremises = [extract(row[1]) for row in test if row[0] in labels]\n",
    "        testHypotheses = [extract(row[2]) for row in test if row[0] in labels]\n",
    "        testLabels = [labels[row[0]] for row in test if row[0] in labels]\n",
    "\n",
    "        valPremises = [extract(row[1]) for row in val if row[0] in labels]\n",
    "        valHypotheses = [extract(row[2]) for row in val if row[0] in labels]\n",
    "        valLabels = [labels[row[0]] for row in val if row[0] in labels]\n",
    "\n",
    "        return [[testPremises, testHypotheses, testLabels], [valPremises, valHypotheses, valLabels]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392702\n",
      "1965\n",
      "7850\n"
     ]
    }
   ],
   "source": [
    "# Reading train and test data\n",
    "\n",
    "datasetPath = \"../../Datasets/MultiNLI/\"\n",
    "trainData = readFileData(f'{datasetPath}multinli_1.0_train.txt', 't')\n",
    "testData, validationData = readFileData(f'{datasetPath}multinli_1.0_dev_matched.txt', 'v')\n",
    "\n",
    "print(len(trainData[0]))\n",
    "print(len(validationData[0]))\n",
    "print(len(testData[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "maxLen = 401\n",
    "epochs = 1000\n",
    "batchSize = 128\n",
    "gloveDimension = 300\n",
    "hiddenDimension = 100\n",
    "regularization = 4e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer to generate the vocabulary of the system\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(trainData[0] + trainData[1])\n",
    "vocabSize = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the train data to sequences as per the vocabulary\n",
    "trainData[0] = tokenizer.texts_to_sequences(trainData[0])\n",
    "trainData[1] = tokenizer.texts_to_sequences(trainData[1])\n",
    "\n",
    "# Pad or trim all generated sequences to the same max sentence length\n",
    "trainData[0] = pad_sequences(trainData[0], maxLen, padding='post')\n",
    "trainData[1] = pad_sequences(trainData[1], maxLen, padding='post')\n",
    "\n",
    "# Transform the labels to one-hot encoding\n",
    "trainData[2] = tf.keras.utils.to_categorical(trainData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test data to sequences as per the vocabulary\n",
    "testData[0] = tokenizer.texts_to_sequences(testData[0])\n",
    "testData[1] = tokenizer.texts_to_sequences(testData[1])\n",
    "\n",
    "# Pad or trim all generated sequences to the same max sentence length\n",
    "testData[0] = pad_sequences(testData[0], maxLen, padding='post')\n",
    "testData[1] = pad_sequences(testData[1], maxLen, padding='post')\n",
    "\n",
    "# Transform the labels to one-hot encoding\n",
    "testData[2] = tf.keras.utils.to_categorical(testData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the validation data to sequences as per the vocabulary\n",
    "validationData[0] = tokenizer.texts_to_sequences(validationData[0])\n",
    "validationData[1] = tokenizer.texts_to_sequences(validationData[1])\n",
    "\n",
    "# Pad or trim all generated sequences to the same max sentence length\n",
    "validationData[0] = pad_sequences(validationData[0], maxLen, padding='post')\n",
    "validationData[1] = pad_sequences(validationData[1], maxLen, padding='post')\n",
    "\n",
    "# Transform the labels to one-hot encoding\n",
    "validationData[2] = tf.keras.utils.to_categorical(validationData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the GloVe embeddings and generate embeddings dictionary\n",
    "\n",
    "embeddingsDict = dict()\n",
    "glovePath = '../../Datasets/GloVe/'\n",
    "glove = open(f'{glovePath}glove.840B.300d.txt', encoding='utf8')\n",
    "\n",
    "for line in glove:\n",
    "    records = line.split()\n",
    "    word = ''.join(records[:-300])\n",
    "    vectorDimensions = asarray(records[-300:], dtype='float32')\n",
    "    embeddingsDict[word] = vectorDimensions\n",
    "\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "5000\n",
      "7500\n",
      "10000\n",
      "12500\n",
      "15000\n",
      "17500\n",
      "20000\n",
      "22500\n",
      "25000\n",
      "27500\n",
      "30000\n",
      "32500\n",
      "35000\n",
      "37500\n",
      "40000\n",
      "42500\n",
      "45000\n",
      "47500\n",
      "50000\n",
      "52500\n",
      "55000\n",
      "57500\n",
      "60000\n",
      "62500\n",
      "65000\n",
      "67500\n",
      "70000\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the embeddings and store only those that are present in our vocabulary\n",
    "embeddingsMat = zeros((vocabSize, gloveDimension))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index % 2500 == 0:\n",
    "        print(index)\n",
    "    vec = embeddingsDict.get(word)\n",
    "    if vec is not None:\n",
    "        embeddingsMat[index] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding layer for our baseline RNN model\n",
    "embed = Embedding(vocabSize, gloveDimension, weights=[embeddingsMat], input_length=maxLen, trainable=False)\n",
    "\n",
    "# As Premise and Hypothesis are distinct and are to be inputted separately, define two inputs and embed\n",
    "premise = Input(shape=(maxLen,), dtype='int32')\n",
    "hypothesis = Input(shape=(maxLen,), dtype='int32')\n",
    "\n",
    "premInput = embed(premise)\n",
    "hypoInput = embed(hypothesis)\n",
    "\n",
    "convert = Dense(hiddenDimension, activation='tanh', input_shape=(gloveDimension,))\n",
    "\n",
    "premInput = convert(premInput)\n",
    "hypoInput = convert(hypoInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the sentence embeddings have been generated, generate a matrix of dimensions maxLen X gloveDimension\n",
    "# On adding maxLen, we get a single embedding vector of length gloveDimension\n",
    "\n",
    "rnn = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=hiddenDimension)\n",
    "# rnn = LSTM(hiddenDimension, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply batch normalization to the two input embeddings separately\n",
    "\n",
    "premInput = rnn(premInput)\n",
    "hypoInput = rnn(hypoInput)\n",
    "premInput = tf.keras.layers.BatchNormalization()(premInput)\n",
    "hypoInput = tf.keras.layers.BatchNormalization()(hypoInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint is a concatenated embeddings layer, generated from the premise and hypothesis inputs\n",
    "# Dilution of probability 0.2, to assist in regularization\n",
    "joint = keras.layers.concatenate([premInput, hypoInput])\n",
    "joint = Dropout(0.2)(joint)\n",
    "for i in range(3):\n",
    "    joint = Dense(2*hiddenDimension, activation='tanh', kernel_regularizer=L2(regularization))(joint)\n",
    "    joint = Dropout(0.2)(joint)\n",
    "    joint = tf.keras.layers.BatchNormalization()(joint)\n",
    "\n",
    "# 3 layers of the TanH activation function, along with L2 regularization.\n",
    "# The final decision is based on the Softmax function\n",
    "pred = Dense(3, activation='softmax')(joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the final models input and output format, as well as compilation parameters\n",
    "\n",
    "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summation Model\n",
    "\n",
    "The following subsection trains the model and reports the findings for the summation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 09:44:38.100954: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 629894008 exceeds 10% of free system memory.\n",
      "2022-05-01 09:44:38.337863: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 629894008 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3068/3068 [==============================] - 225s 59ms/step - loss: 1.0138 - accuracy: 0.4960 - val_loss: 1.4803 - val_accuracy: 0.3608\n",
      "Epoch 2/1000\n",
      "3068/3068 [==============================] - 220s 72ms/step - loss: 0.9257 - accuracy: 0.5622 - val_loss: 0.9590 - val_accuracy: 0.5537\n",
      "Epoch 3/1000\n",
      "3068/3068 [==============================] - 204s 66ms/step - loss: 0.9013 - accuracy: 0.5800 - val_loss: 0.9351 - val_accuracy: 0.5562\n",
      "Epoch 4/1000\n",
      "3068/3068 [==============================] - 189s 62ms/step - loss: 0.8872 - accuracy: 0.5919 - val_loss: 0.8882 - val_accuracy: 0.6000\n",
      "Epoch 5/1000\n",
      "3068/3068 [==============================] - 193s 63ms/step - loss: 0.8759 - accuracy: 0.6009 - val_loss: 0.8550 - val_accuracy: 0.6168\n",
      "Epoch 6/1000\n",
      "3068/3068 [==============================] - 195s 63ms/step - loss: 0.8671 - accuracy: 0.6077 - val_loss: 0.8247 - val_accuracy: 0.6427\n",
      "Epoch 7/1000\n",
      "3068/3068 [==============================] - 195s 63ms/step - loss: 0.8595 - accuracy: 0.6127 - val_loss: 0.8532 - val_accuracy: 0.6219\n",
      "Epoch 8/1000\n",
      "3068/3068 [==============================] - 204s 66ms/step - loss: 0.8551 - accuracy: 0.6170 - val_loss: 0.8475 - val_accuracy: 0.6321\n",
      "Epoch 9/1000\n",
      "3068/3068 [==============================] - 196s 64ms/step - loss: 0.8497 - accuracy: 0.6199 - val_loss: 0.8349 - val_accuracy: 0.6361\n",
      "Epoch 10/1000\n",
      "3068/3068 [==============================] - 194s 63ms/step - loss: 0.8461 - accuracy: 0.6223 - val_loss: 0.8198 - val_accuracy: 0.6438\n",
      "Epoch 11/1000\n",
      "3068/3068 [==============================] - 195s 64ms/step - loss: 0.8417 - accuracy: 0.6264 - val_loss: 0.8197 - val_accuracy: 0.6448\n",
      "Epoch 12/1000\n",
      "3068/3068 [==============================] - 213s 69ms/step - loss: 0.8388 - accuracy: 0.6283 - val_loss: 0.8125 - val_accuracy: 0.6463\n",
      "Epoch 13/1000\n",
      "3068/3068 [==============================] - 215s 70ms/step - loss: 0.8359 - accuracy: 0.6306 - val_loss: 0.8083 - val_accuracy: 0.6458\n",
      "Epoch 14/1000\n",
      "3068/3068 [==============================] - 212s 69ms/step - loss: 0.8344 - accuracy: 0.6322 - val_loss: 0.8048 - val_accuracy: 0.6580\n",
      "Epoch 15/1000\n",
      "3068/3068 [==============================] - 216s 71ms/step - loss: 0.8315 - accuracy: 0.6339 - val_loss: 0.8065 - val_accuracy: 0.6438\n",
      "Epoch 16/1000\n",
      "3068/3068 [==============================] - 217s 71ms/step - loss: 0.8307 - accuracy: 0.6347 - val_loss: 0.7921 - val_accuracy: 0.6585\n",
      "Epoch 17/1000\n",
      "3068/3068 [==============================] - 216s 70ms/step - loss: 0.8283 - accuracy: 0.6359 - val_loss: 0.8173 - val_accuracy: 0.6539\n",
      "Epoch 18/1000\n",
      "3068/3068 [==============================] - 212s 69ms/step - loss: 0.8261 - accuracy: 0.6377 - val_loss: 0.8004 - val_accuracy: 0.6545\n",
      "Epoch 19/1000\n",
      "3068/3068 [==============================] - 195s 64ms/step - loss: 0.8245 - accuracy: 0.6386 - val_loss: 0.7978 - val_accuracy: 0.6687\n",
      "Epoch 20/1000\n",
      "3068/3068 [==============================] - 201s 66ms/step - loss: 0.8234 - accuracy: 0.6397 - val_loss: 0.8027 - val_accuracy: 0.6519\n",
      "Epoch 21/1000\n",
      "3068/3068 [==============================] - 206s 67ms/step - loss: 0.8228 - accuracy: 0.6401 - val_loss: 0.7879 - val_accuracy: 0.6667\n",
      "Epoch 22/1000\n",
      "3068/3068 [==============================] - 215s 70ms/step - loss: 0.8212 - accuracy: 0.6406 - val_loss: 0.8029 - val_accuracy: 0.6560\n",
      "Epoch 23/1000\n",
      "3068/3068 [==============================] - 213s 69ms/step - loss: 0.8207 - accuracy: 0.6418 - val_loss: 0.7938 - val_accuracy: 0.6646\n",
      "Epoch 24/1000\n",
      "3068/3068 [==============================] - 213s 69ms/step - loss: 0.8187 - accuracy: 0.6434 - val_loss: 0.7945 - val_accuracy: 0.6656\n",
      "Epoch 25/1000\n",
      "3068/3068 [==============================] - 213s 69ms/step - loss: 0.8183 - accuracy: 0.6435 - val_loss: 0.7952 - val_accuracy: 0.6494\n",
      "Epoch 26/1000\n",
      "3068/3068 [==============================] - 196s 64ms/step - loss: 0.8170 - accuracy: 0.6437 - val_loss: 0.7891 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4adc21c490>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model using the train data\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n",
    "model.fit([array(trainData[0]), array(trainData[1])], array(trainData[2]), batch_size=batchSize, epochs=epochs, callbacks=[callback], validation_data=[[array(validationData[0]), array(validationData[1])], array(validationData[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1000\n",
      "3068/3068 [==============================] - 101s 33ms/step - loss: 0.8240 - accuracy: 0.6387 - val_loss: 0.8007 - val_accuracy: 0.6509\n",
      "Epoch 2/1000\n",
      "3068/3068 [==============================] - 79s 26ms/step - loss: 0.8222 - accuracy: 0.6405 - val_loss: 0.8005 - val_accuracy: 0.6524\n",
      "Epoch 3/1000\n",
      "3068/3068 [==============================] - 80s 26ms/step - loss: 0.8222 - accuracy: 0.6415 - val_loss: 0.8124 - val_accuracy: 0.6550\n",
      "Epoch 4/1000\n",
      "3068/3068 [==============================] - 83s 27ms/step - loss: 0.8205 - accuracy: 0.6408 - val_loss: 0.8003 - val_accuracy: 0.6494\n",
      "Epoch 5/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8198 - accuracy: 0.6418 - val_loss: 0.7941 - val_accuracy: 0.6560\n",
      "Epoch 6/1000\n",
      "3068/3068 [==============================] - 83s 27ms/step - loss: 0.8190 - accuracy: 0.6433 - val_loss: 0.7979 - val_accuracy: 0.6590\n",
      "Epoch 7/1000\n",
      "3068/3068 [==============================] - 81s 27ms/step - loss: 0.8188 - accuracy: 0.6433 - val_loss: 0.8090 - val_accuracy: 0.6580\n",
      "Epoch 8/1000\n",
      "3068/3068 [==============================] - 83s 27ms/step - loss: 0.8167 - accuracy: 0.6447 - val_loss: 0.8002 - val_accuracy: 0.6478\n",
      "Epoch 9/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8164 - accuracy: 0.6449 - val_loss: 0.8006 - val_accuracy: 0.6565\n",
      "Epoch 10/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8161 - accuracy: 0.6439 - val_loss: 0.8079 - val_accuracy: 0.6601\n",
      "Epoch 11/1000\n",
      "3068/3068 [==============================] - 88s 29ms/step - loss: 0.8140 - accuracy: 0.6460 - val_loss: 0.8181 - val_accuracy: 0.6458\n",
      "Epoch 12/1000\n",
      "3068/3068 [==============================] - 85s 28ms/step - loss: 0.8138 - accuracy: 0.6463 - val_loss: 0.8284 - val_accuracy: 0.6463\n",
      "Epoch 13/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8132 - accuracy: 0.6459 - val_loss: 0.8032 - val_accuracy: 0.6565\n",
      "Epoch 14/1000\n",
      "3068/3068 [==============================] - 88s 29ms/step - loss: 0.8120 - accuracy: 0.6482 - val_loss: 0.7903 - val_accuracy: 0.6646\n",
      "Epoch 15/1000\n",
      "3068/3068 [==============================] - 88s 29ms/step - loss: 0.8125 - accuracy: 0.6474 - val_loss: 0.7969 - val_accuracy: 0.6601\n",
      "Epoch 16/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8113 - accuracy: 0.6473 - val_loss: 0.7860 - val_accuracy: 0.6570\n",
      "Epoch 17/1000\n",
      "3068/3068 [==============================] - 85s 28ms/step - loss: 0.8114 - accuracy: 0.6485 - val_loss: 0.7985 - val_accuracy: 0.6585\n",
      "Epoch 18/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8105 - accuracy: 0.6490 - val_loss: 0.7971 - val_accuracy: 0.6585\n",
      "Epoch 19/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8102 - accuracy: 0.6488 - val_loss: 0.8006 - val_accuracy: 0.6585\n",
      "Epoch 20/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8095 - accuracy: 0.6493 - val_loss: 0.8016 - val_accuracy: 0.6555\n",
      "Epoch 21/1000\n",
      "3068/3068 [==============================] - 87s 28ms/step - loss: 0.8082 - accuracy: 0.6497 - val_loss: 0.7942 - val_accuracy: 0.6570\n",
      "Epoch 22/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8084 - accuracy: 0.6503 - val_loss: 0.8053 - val_accuracy: 0.6555\n",
      "Epoch 23/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8082 - accuracy: 0.6498 - val_loss: 0.8111 - val_accuracy: 0.6565\n",
      "Epoch 24/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8064 - accuracy: 0.6511 - val_loss: 0.7895 - val_accuracy: 0.6692\n",
      "Epoch 25/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8079 - accuracy: 0.6503 - val_loss: 0.7901 - val_accuracy: 0.6595\n",
      "Epoch 26/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8058 - accuracy: 0.6514 - val_loss: 0.8040 - val_accuracy: 0.6514\n",
      "Epoch 27/1000\n",
      "3068/3068 [==============================] - 87s 28ms/step - loss: 0.8054 - accuracy: 0.6522 - val_loss: 0.7983 - val_accuracy: 0.6570\n",
      "Epoch 28/1000\n",
      "3068/3068 [==============================] - 88s 29ms/step - loss: 0.8069 - accuracy: 0.6513 - val_loss: 0.7899 - val_accuracy: 0.6651\n",
      "Epoch 29/1000\n",
      "3068/3068 [==============================] - 87s 28ms/step - loss: 0.8049 - accuracy: 0.6527 - val_loss: 0.8037 - val_accuracy: 0.6621\n",
      "Epoch 30/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8048 - accuracy: 0.6519 - val_loss: 0.7860 - val_accuracy: 0.6662\n",
      "Epoch 31/1000\n",
      "3068/3068 [==============================] - 85s 28ms/step - loss: 0.8037 - accuracy: 0.6537 - val_loss: 0.8016 - val_accuracy: 0.6626\n",
      "Epoch 32/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8042 - accuracy: 0.6523 - val_loss: 0.7904 - val_accuracy: 0.6651\n",
      "Epoch 33/1000\n",
      "3068/3068 [==============================] - 87s 28ms/step - loss: 0.8039 - accuracy: 0.6538 - val_loss: 0.7919 - val_accuracy: 0.6626\n",
      "Epoch 34/1000\n",
      "3068/3068 [==============================] - 85s 28ms/step - loss: 0.8036 - accuracy: 0.6540 - val_loss: 0.7973 - val_accuracy: 0.6560\n",
      "Epoch 35/1000\n",
      "3068/3068 [==============================] - 85s 28ms/step - loss: 0.8030 - accuracy: 0.6537 - val_loss: 0.7799 - val_accuracy: 0.6697\n",
      "Epoch 36/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8035 - accuracy: 0.6528 - val_loss: 0.7938 - val_accuracy: 0.6626\n",
      "Epoch 37/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8023 - accuracy: 0.6538 - val_loss: 0.7886 - val_accuracy: 0.6707\n",
      "Epoch 38/1000\n",
      "3068/3068 [==============================] - 187s 61ms/step - loss: 0.8019 - accuracy: 0.6545 - val_loss: 0.7942 - val_accuracy: 0.6601\n",
      "Epoch 39/1000\n",
      "3068/3068 [==============================] - 86s 28ms/step - loss: 0.8017 - accuracy: 0.6541 - val_loss: 0.7911 - val_accuracy: 0.6718\n",
      "Epoch 40/1000\n",
      "3068/3068 [==============================] - 215s 70ms/step - loss: 0.8021 - accuracy: 0.6539 - val_loss: 0.7844 - val_accuracy: 0.6723\n",
      "\n",
      "<keras.callbacks.History at 0x5b63aa24f099>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model using the train data\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=15)\n",
    "model.fit([array(trainData[0]), array(trainData[1])], array(trainData[2]), batch_size=batchSize, epochs=epochs, callbacks=[callback], validation_data=[[array(validationData[0]), array(validationData[1])], array(validationData[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 2s 60ms/step - loss: 0.7936 - accuracy: 0.6575\n",
      "Loss =  0.7935532331466675\n",
      "Acc =  0.6574522256851196\n"
     ]
    }
   ],
   "source": [
    "# Evaluating accuracy on the trained model\n",
    "\n",
    "loss, acc = model.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
    "print('Loss = ', loss)\n",
    "print('Acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../Models/summation_multiNLI.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccc = keras.models.load_model('../../Models/summation_multiNLI.h5')\n",
    "l, a = ccc.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following results for the summation model, for the SNLI dataset:\n",
    "\n",
    "- Training Accuracy: 65.39%\n",
    "- Validation Accuracy: 67.23%\n",
    "- Test Accuracy: 65.74%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "\n",
    "The following subsection trains the model and reports the findings for the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1000\n",
      "384/384 [==============================] - 313s 815ms/step - loss: 0.8192 - accuracy: 0.6032 - val_loss: 0.9028 - val_accuracy: 0.6277\n",
      "Epoch 2/1000\n",
      "384/384 [==============================] - 311s 810ms/step - loss: 0.8033 - accuracy: 0.6039 - val_loss: 0.9111 - val_accuracy: 0.6125\n",
      "Epoch 3/1000\n",
      "384/384 [==============================] - 311s 811ms/step - loss: 0.8028 - accuracy: 0.6110 - val_loss: 0.8004 - val_accuracy: 0.6277\n",
      "Epoch 4/1000\n",
      "384/384 [==============================] - 312s 812ms/step - loss: 0.8025 - accuracy: 0.6173 - val_loss: 0.8022 - val_accuracy: 0.6277\n",
      "Epoch 5/1000\n",
      "384/384 [==============================] - 311s 811ms/step - loss: 0.8023 - accuracy: 0.6239 - val_loss: 0.8038 - val_accuracy: 0.6272\n",
      "Epoch 6/1000\n",
      "384/384 [==============================] - 312s 811ms/step - loss: 0.8019 - accuracy: 0.6248 - val_loss: 0.8061 - val_accuracy: 0.6125\n",
      "Epoch 7/1000\n",
      "384/384 [==============================] - 312s 812ms/step - loss: 0.8018 - accuracy: 0.6294 - val_loss: 0.8996 - val_accuracy: 0.6593\n",
      "Epoch 8/1000\n",
      "384/384 [==============================] - 312s 811ms/step - loss: 0.8016 - accuracy: 0.6330 - val_loss: 0.8991 - val_accuracy: 0.6598\n",
      "Epoch 9/1000\n",
      "384/384 [==============================] - 311s 811ms/step - loss: 0.8013 - accuracy: 0.6386 - val_loss: 0.8990 - val_accuracy: 0.6598\n",
      "Epoch 10/1000\n",
      "384/384 [==============================] - 312s 811ms/step - loss: 0.8009 - accuracy: 0.6427 - val_loss: 0.8986 - val_accuracy: 0.6598\n",
      "Epoch 11/1000\n",
      "384/384 [==============================] - 312s 812ms/step - loss: 0.8006 - accuracy: 0.6434 - val_loss: 0.8021 - val_accuracy: 0.6125\n",
      "Epoch 12/1000\n",
      "384/384 [==============================] - 311s 811ms/step - loss: 0.8004 - accuracy: 0.6430 - val_loss: 0.8020 - val_accuracy: 0.6277\n",
      "Epoch 13/1000\n",
      "384/384 [==============================] - 312s 811ms/step - loss: 0.8002 - accuracy: 0.6471 - val_loss: 0.8990 - val_accuracy: 0.6277\n",
      "Epoch 14/1000\n",
      "384/384 [==============================] - 311s 811ms/step - loss: 0.7998 - accuracy: 0.6540 - val_loss: 0.8978 - val_accuracy: 0.6598\n",
      "Epoch 15/1000\n",
      "384/384 [==============================] - 311s 810ms/step - loss: 0.7996 - accuracy: 0.6536 - val_loss: 0.8999 - val_accuracy: 0.6125\n",
      "Epoch 16/1000\n",
      "384/384 [==============================] - 311s 810ms/step - loss: 0.7993 - accuracy: 0.6625 - val_loss: 0.8991 - val_accuracy: 0.6277\n",
      "Epoch 17/1000\n",
      "384/384 [==============================] - 311s 809ms/step - loss: 0.7991 - accuracy: 0.6635 - val_loss: 0.8001 - val_accuracy: 0.6125\n",
      "Epoch 18/1000\n",
      "384/384 [==============================] - 311s 809ms/step - loss: 0.7989 - accuracy: 0.6734 - val_loss: 0.8993 - val_accuracy: 0.6125\n",
      "Epoch 19/1000\n",
      "384/384 [==============================] - 311s 810ms/step - loss: 0.7988 - accuracy: 0.6741 - val_loss: 0.8981 - val_accuracy: 0.6598\n",
      "\n",
      "<keras.callbacks.History at 0x6c63ba75f17a>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model using the train data\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n",
    "model.fit([array(trainData[0]), array(trainData[1])], array(trainData[2]), batch_size=batchSize, epochs=epochs, callbacks=[callback], validation_data=[[array(validationData[0]), array(validationData[1])], array(validationData[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../Models/LSTM_multiNLI.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../../Models/LSTM_multiNLI.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "39/39 [==============================] - 2s 60ms/step - loss: 0.8936 - accuracy: 0.6576\n",
      "Loss =  0.8935532331466675\n",
      "Acc =  0.6576725454179533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating accuracy on the trained model\n",
    "\n",
    "loss, acc = model.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
    "print('Loss = ', loss)\n",
    "print('Acc = ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus observe the following results for the simple LSTM model, for the MultiNLI dataset:\n",
    "\n",
    "- Training Accuracy: 67.41%\n",
    "- Validation Accuracy: 65.98%\n",
    "- Test Accuracy: 65.76%"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
