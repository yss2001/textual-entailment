{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shared LSTM Model - MultiNLI Dataset\n",
        "\n",
        "The following notebook contains the implementation of the shared LSTM model for the MultiNLI dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkfB55GS6ZnP",
        "outputId": "9e54925b-f09e-40ff-f9c8-c52821fbd86f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/yss/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "\n",
        "import re\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import L2\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Dropout, Input, LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from gensim import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4DjPgbVd6ZnT"
      },
      "outputs": [],
      "source": [
        "# Helper function that cleans the input data and enumerates the labels\n",
        "\n",
        "def extract(s):\n",
        "    s = re.sub('\\\\(', '', s)\n",
        "    s = re.sub('\\\\)', '', s)\n",
        "    s = re.sub('\\\\s{2,}', ' ', s)\n",
        "    return s.strip()\n",
        "\n",
        "labels = {'entailment': 0, 'contradiction': 1, 'neutral': 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8jwY84sY6ZnU"
      },
      "outputs": [],
      "source": [
        "# Function that reads data and parses data from file\n",
        "\n",
        "def readFileData(filePath, t):\n",
        "    with open(filePath, 'r') as f:\n",
        "        inputRows = [row.split('\\t') for row in f.readlines()[1:]]\n",
        "\n",
        "    \n",
        "    if t == 't':\n",
        "        inputPremises = [extract(row[1]) for row in inputRows if row[0] in labels]\n",
        "        inputHypotheses = [extract(row[2]) for row in inputRows if row[0] in labels]\n",
        "        inputLabels = [labels[row[0]] for row in inputRows if row[0] in labels]\n",
        "        f.close()\n",
        "\n",
        "        return [inputPremises, inputHypotheses, inputLabels]\n",
        "\n",
        "    else:\n",
        "        test, val = train_test_split(inputRows, test_size=0.2, random_state=42)\n",
        "        \n",
        "        testPremises = [extract(row[1]) for row in test if row[0] in labels]\n",
        "        testHypotheses = [extract(row[2]) for row in test if row[0] in labels]\n",
        "        testLabels = [labels[row[0]] for row in test if row[0] in labels]\n",
        "\n",
        "        valPremises = [extract(row[1]) for row in val if row[0] in labels]\n",
        "        valHypotheses = [extract(row[2]) for row in val if row[0] in labels]\n",
        "        valLabels = [labels[row[0]] for row in val if row[0] in labels]\n",
        "\n",
        "        return [[testPremises, testHypotheses, testLabels], [valPremises, valHypotheses, valLabels]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F4nBYHiz6ZnV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "392702\n",
            "1965\n",
            "7850\n"
          ]
        }
      ],
      "source": [
        "# Reading train and test data\n",
        "\n",
        "datasetPath = \"../../Datasets/MultiNLI/\"\n",
        "trainData = readFileData(f'{datasetPath}multinli_1.0_train.txt', 't')\n",
        "testData, validationData = readFileData(f'{datasetPath}multinli_1.0_dev_matched.txt', 'v')\n",
        "\n",
        "print(len(trainData[0]))\n",
        "print(len(validationData[0]))\n",
        "print(len(testData[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cIVwthL46ZnV"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "maxLen = 401\n",
        "epochs = 1000\n",
        "batchSize = 128\n",
        "gloveDimension = 300\n",
        "hiddenDimension = 100\n",
        "regularization = 4e-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "n0X9TGPc6ZnW"
      },
      "outputs": [],
      "source": [
        "# Tokenizer to generate the vocabulary of the system\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(trainData[0] + trainData[1])\n",
        "vocabSize = len(tokenizer.word_index)+1\n",
        "#vocabSize = 35001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DLqheMvs6ZnW"
      },
      "outputs": [],
      "source": [
        "# Convert the train data to sequences as per the vocabulary\n",
        "trainData[0] = tokenizer.texts_to_sequences(trainData[0])\n",
        "trainData[1] = tokenizer.texts_to_sequences(trainData[1])\n",
        "\n",
        "# Pad or trim all generated sequences to the same max sentence length\n",
        "trainData[0] = pad_sequences(trainData[0], maxLen, padding='post')\n",
        "trainData[1] = pad_sequences(trainData[1], maxLen, padding='post')\n",
        "\n",
        "# Transform the labels to one-hot encoding\n",
        "trainData[2] = tf.keras.utils.to_categorical(trainData[2], num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6QeVNUhh6ZnW"
      },
      "outputs": [],
      "source": [
        "# Convert the test data to sequences as per the vocabulary\n",
        "testData[0] = tokenizer.texts_to_sequences(testData[0])\n",
        "testData[1] = tokenizer.texts_to_sequences(testData[1])\n",
        "\n",
        "# Pad or trim all generated sequences to the same max sentence length\n",
        "testData[0] = pad_sequences(testData[0], maxLen, padding='post')\n",
        "testData[1] = pad_sequences(testData[1], maxLen, padding='post')\n",
        "\n",
        "# Transform the labels to one-hot encoding\n",
        "testData[2] = tf.keras.utils.to_categorical(testData[2], num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Z6vN69Pj6ZnX"
      },
      "outputs": [],
      "source": [
        "# Convert the validation data to sequences as per the vocabulary\n",
        "validationData[0] = tokenizer.texts_to_sequences(validationData[0])\n",
        "validationData[1] = tokenizer.texts_to_sequences(validationData[1])\n",
        "\n",
        "# Pad or trim all generated sequences to the same max sentence length\n",
        "validationData[0] = pad_sequences(validationData[0], maxLen, padding='post')\n",
        "validationData[1] = pad_sequences(validationData[1], maxLen, padding='post')\n",
        "\n",
        "# Transform the labels to one-hot encoding\n",
        "validationData[2] = tf.keras.utils.to_categorical(validationData[2], num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the embeddings dictionary from Google News word2vec\n",
        "\n",
        "word2vecpath = \"../../Datasets/Word2Vec/\"\n",
        "embeddingsDict = models.KeyedVectors.load_word2vec_format(f'{word2vecpath}GoogleNews-vectors-negative300.bin', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNWQ50Fq6ZnY"
      },
      "outputs": [],
      "source": [
        "# Alternative GloVe embeddings\n",
        "\n",
        "'''\n",
        "embeddingsDict = dict()\n",
        "glove = open(r'/content/drive/MyDrive/glove.840B.300d.txt', encoding='utf8')\n",
        "\n",
        "for line in glove:\n",
        "    records = line.split()\n",
        "    word = ''.join(records[:-300])\n",
        "    vectorDimensions = asarray(records[-300:], dtype='float32')\n",
        "    embeddingsDict[word] = vectorDimensions\n",
        "\n",
        "glove.close()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dwPl7uUN6ZnY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2500\n",
            "5000\n",
            "7500\n",
            "10000\n",
            "12500\n",
            "15000\n",
            "17500\n",
            "20000\n",
            "22500\n",
            "25000\n",
            "27500\n",
            "30000\n",
            "32500\n",
            "35000\n",
            "37500\n",
            "40000\n",
            "42500\n",
            "45000\n",
            "47500\n",
            "50000\n",
            "52500\n",
            "55000\n",
            "57500\n",
            "60000\n",
            "62500\n",
            "65000\n",
            "67500\n",
            "70000\n"
          ]
        }
      ],
      "source": [
        "# Iterate through the embeddings and store only those that are present in our vocabulary\n",
        "'''\n",
        "embeddingsMat = zeros((vocabSize, gloveDimension))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    vec = embeddingsDict.get(word)\n",
        "    if vec is not None:\n",
        "        embeddingsMat[index] = vec\n",
        "'''\n",
        "\n",
        "embeddingsMat = zeros((vocabSize, gloveDimension))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index % 2500 == 0:\n",
        "        print(index)\n",
        "    if word in embeddingsDict.index_to_key:\n",
        "        embeddingsMat[index] = embeddingsDict[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZXbOSpB56ZnY"
      },
      "outputs": [],
      "source": [
        "# Define the embedding layer for our baseline RNN model\n",
        "embed = Embedding(vocabSize, gloveDimension, weights=[embeddingsMat], input_length=maxLen, trainable=False, mask_zero=True)\n",
        "\n",
        "# As Premise and Hypothesis are distinct and are to be inputted separately, define two inputs and embed\n",
        "premise = Input(shape=(maxLen,), dtype='int32')\n",
        "hypothesis = Input(shape=(maxLen,), dtype='int32')\n",
        "\n",
        "premInput = embed(premise)\n",
        "hypoInput = embed(hypothesis)\n",
        "\n",
        "convert = Dense(hiddenDimension, activation='tanh', input_shape=(gloveDimension,))\n",
        "\n",
        "premInput = convert(premInput)\n",
        "hypoInput = convert(hypoInput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pJ6YggVR6ZnZ"
      },
      "outputs": [],
      "source": [
        "# Once the sentence embeddings have been generated, generate a matrix of dimensions maxLen X gloveDimension\n",
        "# On adding maxLen, we get a single embedding vector of length gloveDimension\n",
        "\n",
        "#rnn = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=hiddenDimension)\n",
        "#rnn = LSTM(hiddenDimension, dropout=0.2, recurrent_dropout=0.2)\n",
        "\n",
        "rnnH = LSTM(hiddenDimension, dropout=0.2, return_state=True)\n",
        "rnnP = LSTM(hiddenDimension, dropout=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8fZbzv_-6ZnZ"
      },
      "outputs": [],
      "source": [
        "# Apply batch normalization to the two input embeddings separately\n",
        "\n",
        "premInput, sH, sC = rnnH(premInput)\n",
        "hypoInput = rnnP(hypoInput, initial_state=[sH, sC])\n",
        "premInput = tf.keras.layers.BatchNormalization()(premInput)\n",
        "result = tf.keras.layers.BatchNormalization()(hypoInput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gwe0F0N-6ZnZ"
      },
      "outputs": [],
      "source": [
        "# Joint is a concatenated embeddings layer, generated from the premise and hypothesis inputs\n",
        "# Dilution of probability 0.2, to assist in regularization\n",
        "#joint = keras.layers.concatenate([premInput, hypoInput])\n",
        "result = Dropout(0.2)(result)\n",
        "'''\n",
        "for i in range(3):\n",
        "    result = Dense(2*hiddenDimension, activation='tanh', kernel_regularizer=L2(regularization))(result)\n",
        "    result = Dropout(0.2)(result)\n",
        "    result = tf.keras.layers.BatchNormalization()(result)\n",
        "'''\n",
        "\n",
        "# 3 layers of the TanH activation function, along with L2 regularization.\n",
        "# The final decision is based on the Softmax function\n",
        "\n",
        "result = Dense(hiddenDimension, activation='tanh', kernel_regularizer=L2(regularization))(result)\n",
        "pred = Dense(3, activation='softmax')(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Q4XPOMBB6Zna"
      },
      "outputs": [],
      "source": [
        "# Defining the final models input and output format, as well as compilation parameters\n",
        "\n",
        "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OSsOhds56Zna"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/1000\n",
            "3068/3068 [==============================] - 345s 107ms/step - loss: 0.9350 - accuracy: 0.5453 - val_loss: 0.8646 - val_accuracy: 0.6122\n",
            "Epoch 2/1000\n",
            "3068/3068 [==============================] - 339s 111ms/step - loss: 0.8500 - accuracy: 0.6091 - val_loss: 0.8065 - val_accuracy: 0.6463\n",
            "Epoch 3/1000\n",
            "3068/3068 [==============================] - 333s 108ms/step - loss: 0.8182 - accuracy: 0.6307 - val_loss: 0.7874 - val_accuracy: 0.6499\n",
            "Epoch 4/1000\n",
            "3068/3068 [==============================] - 328s 107ms/step - loss: 0.7970 - accuracy: 0.6444 - val_loss: 0.7797 - val_accuracy: 0.6529\n",
            "Epoch 5/1000\n",
            "3068/3068 [==============================] - 326s 106ms/step - loss: 0.7811 - accuracy: 0.6538 - val_loss: 0.7636 - val_accuracy: 0.6606\n",
            "Epoch 6/1000\n",
            "3068/3068 [==============================] - 327s 107ms/step - loss: 0.7673 - accuracy: 0.6624 - val_loss: 0.7497 - val_accuracy: 0.6794\n",
            "Epoch 7/1000\n",
            "3068/3068 [==============================] - 327s 107ms/step - loss: 0.7551 - accuracy: 0.6694 - val_loss: 0.7585 - val_accuracy: 0.6728\n",
            "Epoch 8/1000\n",
            "3068/3068 [==============================] - 332s 108ms/step - loss: 0.7458 - accuracy: 0.6745 - val_loss: 0.7679 - val_accuracy: 0.6667\n",
            "Epoch 9/1000\n",
            "3068/3068 [==============================] - 343s 112ms/step - loss: 0.7376 - accuracy: 0.6794 - val_loss: 0.7558 - val_accuracy: 0.6748\n",
            "Epoch 10/1000\n",
            "3068/3068 [==============================] - 351s 114ms/step - loss: 0.7300 - accuracy: 0.6834 - val_loss: 0.7496 - val_accuracy: 0.6718\n",
            "Epoch 11/1000\n",
            "3068/3068 [==============================] - 331s 108ms/step - loss: 0.7239 - accuracy: 0.6873 - val_loss: 0.7422 - val_accuracy: 0.6809\n",
            "Epoch 12/1000\n",
            "3068/3068 [==============================] - 328s 107ms/step - loss: 0.7178 - accuracy: 0.6903 - val_loss: 0.7391 - val_accuracy: 0.6885\n",
            "Epoch 13/1000\n",
            "3068/3068 [==============================] - 326s 106ms/step - loss: 0.7119 - accuracy: 0.6944 - val_loss: 0.7415 - val_accuracy: 0.6931\n",
            "Epoch 14/1000\n",
            "3068/3068 [==============================] - 330s 107ms/step - loss: 0.7071 - accuracy: 0.6961 - val_loss: 0.7500 - val_accuracy: 0.6880\n",
            "Epoch 15/1000\n",
            "3068/3068 [==============================] - 329s 107ms/step - loss: 0.7011 - accuracy: 0.7005 - val_loss: 0.7524 - val_accuracy: 0.6799\n",
            "Epoch 16/1000\n",
            "3068/3068 [==============================] - 331s 108ms/step - loss: 0.6994 - accuracy: 0.7016 - val_loss: 0.7419 - val_accuracy: 0.6840\n",
            "Epoch 17/1000\n",
            "3068/3068 [==============================] - 331s 108ms/step - loss: 0.6953 - accuracy: 0.7029 - val_loss: 0.7542 - val_accuracy: 0.6865\n",
            "\n",
            "<keras.callbacks.History at 0x7f7f5136f6a0>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fitting the model using the train data\n",
        "\n",
        "callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n",
        "mcCallback = ModelCheckpoint('./2lstmMultiMatch', monitor='val_loss')\n",
        "model.fit([array(trainData[0]), array(trainData[1])], array(trainData[2]), batch_size=batchSize, epochs=epochs, callbacks=[callback, mcCallback], validation_data=[[array(validationData[0]), array(validationData[1])], array(validationData[2])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('../../Models/2LSTM_multiNLI.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "C58V0Ch76Znb",
        "outputId": "7f877f90-f67f-4eaa-a940-70268bd9b5d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 4s 143ms/step - loss: 0.7634 - accuracy: 0.6748\n",
            "Loss =  0.7633659243583679\n",
            "Acc =  0.674777090549469\n"
          ]
        }
      ],
      "source": [
        "# Evaluating accuracy on the trained model\n",
        "\n",
        "loss, acc = model.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
        "print('Loss = ', loss)\n",
        "print('Acc = ', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8JfbMdk6Znb",
        "outputId": "6fe423f6-c8a9-4628-c89a-762f6ab220ac"
      },
      "outputs": [],
      "source": [
        "ccc = keras.models.load_model('../../Models/2LSTM_multiNLI.h5')\n",
        "l, a = ccc.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can thus observe the following results for the shared LSTM model, for the SNLI dataset:\n",
        "\n",
        "- Training Accuracy: 70.29%\n",
        "- Validation Accuracy: 68.65%\n",
        "- Test Accuracy: 67.47%\n",
        "\n",
        "Here, we see that even with shared LSTM, we aren't able to get good results for the MultiNLI results, and its increase over simple LSTM is only minimal, as compared to the SNLI dataset.\n",
        "This comes down to the nature of the dataset. The MultiNLI dataset is much more diverse and varied in terms of context and genres of entailment, and it thus has more difficulties in picking up the language semantics required for proper NLI classification. This can be resolved via better embeddings, as the word2vec embeddings chosen here are relatively specific, or having a larger model with bigger inputs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of summationBowman.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
