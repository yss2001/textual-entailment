{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summation & LSTM Model - SNLI Dataset\n",
    "\n",
    "The following notebook contains the implementation of the baseline summation model, and the standard LSTM model for the SNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/aakashj2412/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import re\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.regularizers import L2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, Input, LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that cleans the input data and enumerates the labels\n",
    "\n",
    "def extract(s):\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "labels = {'entailment': 0, 'contradiction': 1, 'neutral': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that reads data and parses data from file\n",
    "\n",
    "def readFileData(filePath):\n",
    "    with open(filePath, 'r') as f:\n",
    "        inputRows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    inputPremises = [extract(row[1]) for row in inputRows if row[0] in labels]\n",
    "    inputHypotheses = [extract(row[2]) for row in inputRows if row[0] in labels]\n",
    "    inputLabels = [labels[row[0]] for row in inputRows if row[0] in labels]\n",
    "    f.close()\n",
    "\n",
    "    return [inputPremises, inputHypotheses, inputLabels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train and test data\n",
    "\n",
    "datasetPath = \"../../Datasets/SNLI/\"\n",
    "trainData = readFileData(f'{datasetPath}snli_1.0_train.txt')\n",
    "testData = readFileData(f'{datasetPath}snli_1.0_test.txt')\n",
    "validationData = readFileData(f'{datasetPath}snli_1.0_dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "maxLen = 50\n",
    "epochs = 1000\n",
    "batchSize = 128\n",
    "gloveDimension = 300\n",
    "hiddenDimension = 100\n",
    "regularization = 4e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer to generate the vocabulary of the system\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(trainData[0] + trainData[1])\n",
    "vocabSize = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the train data to sequences as per the vocabulary\n",
    "trainData[0] = tokenizer.texts_to_sequences(trainData[0])\n",
    "trainData[1] = tokenizer.texts_to_sequences(trainData[1])\n",
    "\n",
    "# Pad or trim all generated sequences to the same max sentence length\n",
    "trainData[0] = pad_sequences(trainData[0], maxLen, padding='post')\n",
    "trainData[1] = pad_sequences(trainData[1], maxLen, padding='post')\n",
    "\n",
    "# Transform the labels to one-hot encoding\n",
    "trainData[2] = tf.keras.utils.to_categorical(trainData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test data to sequences as per the vocabulary\n",
    "testData[0] = tokenizer.texts_to_sequences(testData[0])\n",
    "testData[1] = tokenizer.texts_to_sequences(testData[1])\n",
    "\n",
    "# Pad or trim all generated sequences to the same max sentence length\n",
    "testData[0] = pad_sequences(testData[0], maxLen, padding='post')\n",
    "testData[1] = pad_sequences(testData[1], maxLen, padding='post')\n",
    "\n",
    "# Transform the labels to one-hot encoding\n",
    "testData[2] = tf.keras.utils.to_categorical(testData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the validation data to sequences as per the vocabulary\n",
    "validationData[0] = tokenizer.texts_to_sequences(validationData[0])\n",
    "validationData[1] = tokenizer.texts_to_sequences(validationData[1])\n",
    "\n",
    "# Pad or trim all generated sequences to the same max sentence length\n",
    "validationData[0] = pad_sequences(validationData[0], maxLen, padding='post')\n",
    "validationData[1] = pad_sequences(validationData[1], maxLen, padding='post')\n",
    "\n",
    "# Transform the labels to one-hot encoding\n",
    "validationData[2] = tf.keras.utils.to_categorical(validationData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the GloVe embeddings and generate embeddings dictionary\n",
    "\n",
    "embeddingsDict = dict()\n",
    "glovePath = '../../Datasets/GloVe/'\n",
    "glove = open(f'{glovePath}glove.840B.300d.txt', encoding='utf8')\n",
    "\n",
    "for line in glove:\n",
    "    records = line.split()\n",
    "    word = ''.join(records[:-300])\n",
    "    vectorDimensions = asarray(records[-300:], dtype='float32')\n",
    "    embeddingsDict[word] = vectorDimensions\n",
    "\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the embeddings and store only those that are present in our vocabulary\n",
    "\n",
    "embeddingsMat = zeros((vocabSize, gloveDimension))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    vec = embeddingsDict.get(word)\n",
    "    if vec is not None:\n",
    "        embeddingsMat[index] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding layer for our baseline RNN model\n",
    "embed = Embedding(vocabSize, gloveDimension, weights=[embeddingsMat], input_length=maxLen, trainable=False)\n",
    "\n",
    "# As Premise and Hypothesis are distinct and are to be inputted separately, define two inputs and embed\n",
    "premise = Input(shape=(maxLen,), dtype='int32')\n",
    "hypothesis = Input(shape=(maxLen,), dtype='int32')\n",
    "\n",
    "premInput = embed(premise)\n",
    "hypoInput = embed(hypothesis)\n",
    "\n",
    "convert = Dense(hiddenDimension, activation='tanh', input_shape=(gloveDimension,))\n",
    "\n",
    "premInput = convert(premInput)\n",
    "hypoInput = convert(hypoInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the sentence embeddings have been generated, generate a matrix of dimensions maxLen X gloveDimension\n",
    "# On adding maxLen, we get a single embedding vector of length gloveDimension\n",
    "\n",
    "# Alternate between #1 and #2 rnn definitions to switch between summation and LSTM model\n",
    "\n",
    "rnn = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=hiddenDimension)\n",
    "# rnn = LSTM(hiddenDimension, dropout=0.2, recurrent_dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply batch normalization to the two input embeddings separately\n",
    "\n",
    "premInput = rnn(premInput)\n",
    "hypoInput = rnn(hypoInput)\n",
    "premInput = tf.keras.layers.BatchNormalization()(premInput)\n",
    "hypoInput = tf.keras.layers.BatchNormalization()(hypoInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint is a concatenated embeddings layer, generated from the premise and hypothesis inputs\n",
    "# Dilution of probability 0.2, to assist in regularization\n",
    "joint = keras.layers.concatenate([premInput, hypoInput])\n",
    "joint = Dropout(0.2)(joint)\n",
    "for i in range(3):\n",
    "    joint = Dense(2*hiddenDimension, activation='tanh', kernel_regularizer=L2(regularization))(joint)\n",
    "    joint = Dropout(0.2)(joint)\n",
    "    joint = tf.keras.layers.BatchNormalization()(joint)\n",
    "\n",
    "# 3 layers of the TanH activation function, along with L2 regularization.\n",
    "# The final decision is based on the Softmax function\n",
    "pred = Dense(3, activation='softmax')(joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the final models input and output format, as well as compilation parameters\n",
    "\n",
    "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summation Model\n",
    "\n",
    "The following subsection trains the model and reports the findings for the summation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model using the train data\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n",
    "model.fit([array(trainData[0]), array(trainData[1])], array(trainData[2]), batch_size=batchSize, epochs=epochs, callbacks=[callback], validation_data=[[array(validationData[0]), array(validationData[1])], array(validationData[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating accuracy on the trained model\n",
    "\n",
    "loss, acc = model.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
    "print('Loss = ', loss)\n",
    "print('Acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../Models/summation_snli.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccc = keras.models.load_model('../../Models/summation_snli.h5')\n",
    "l, a = ccc.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following results for the summation model, for the SNLI dataset:\n",
    "\n",
    "- Training Accuracy: 74%\n",
    "- Validation Accuracy: 77%\n",
    "- Test Accuracy: 76%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "\n",
    "The following subsection trains the model and reports the findings for the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4292/4292 [==============================] - 697s 160ms/step - loss: 0.9132 - accuracy: 0.5727 - val_loss: 0.8154 - val_accuracy: 0.6387\n",
      "Epoch 2/1000\n",
      "4292/4292 [==============================] - 720s 168ms/step - loss: 0.7878 - accuracy: 0.6549 - val_loss: 0.7282 - val_accuracy: 0.6915\n",
      "Epoch 3/1000\n",
      "4292/4292 [==============================] - 735s 171ms/step - loss: 0.7381 - accuracy: 0.6858 - val_loss: 0.6901 - val_accuracy: 0.7122\n",
      "Epoch 4/1000\n",
      "4292/4292 [==============================] - 706s 164ms/step - loss: 0.7120 - accuracy: 0.7008 - val_loss: 0.6574 - val_accuracy: 0.7291\n",
      "Epoch 5/1000\n",
      "4292/4292 [==============================] - 692s 161ms/step - loss: 0.6956 - accuracy: 0.7095 - val_loss: 0.6401 - val_accuracy: 0.7395\n",
      "Epoch 6/1000\n",
      "4292/4292 [==============================] - 694s 162ms/step - loss: 0.6836 - accuracy: 0.7168 - val_loss: 0.6359 - val_accuracy: 0.7431\n",
      "Epoch 7/1000\n",
      "4292/4292 [==============================] - 695s 162ms/step - loss: 0.6728 - accuracy: 0.7229 - val_loss: 0.6217 - val_accuracy: 0.7444\n",
      "Epoch 8/1000\n",
      "4292/4292 [==============================] - 695s 162ms/step - loss: 0.6645 - accuracy: 0.7273 - val_loss: 0.6176 - val_accuracy: 0.7524\n",
      "Epoch 9/1000\n",
      "4292/4292 [==============================] - 746s 174ms/step - loss: 0.6569 - accuracy: 0.7315 - val_loss: 0.6046 - val_accuracy: 0.7607\n",
      "Epoch 10/1000\n",
      "4292/4292 [==============================] - 754s 176ms/step - loss: 0.6515 - accuracy: 0.7349 - val_loss: 0.5978 - val_accuracy: 0.7634\n",
      "Epoch 11/1000\n",
      "4292/4292 [==============================] - 733s 171ms/step - loss: 0.6466 - accuracy: 0.7372 - val_loss: 0.5942 - val_accuracy: 0.7635\n",
      "Epoch 12/1000\n",
      "4292/4292 [==============================] - 738s 172ms/step - loss: 0.6426 - accuracy: 0.7398 - val_loss: 0.5900 - val_accuracy: 0.7673\n",
      "Epoch 13/1000\n",
      "4292/4292 [==============================] - 789s 184ms/step - loss: 0.6392 - accuracy: 0.7417 - val_loss: 0.5824 - val_accuracy: 0.7716\n",
      "Epoch 14/1000\n",
      "4292/4292 [==============================] - 1065s 248ms/step - loss: 0.6370 - accuracy: 0.7426 - val_loss: 0.5809 - val_accuracy: 0.7731\n",
      "Epoch 15/1000\n",
      "4292/4292 [==============================] - 981s 228ms/step - loss: 0.6326 - accuracy: 0.7452 - val_loss: 0.5784 - val_accuracy: 0.7751\n",
      "Epoch 16/1000\n",
      "4292/4292 [==============================] - 757s 176ms/step - loss: 0.6316 - accuracy: 0.7455 - val_loss: 0.5738 - val_accuracy: 0.7748\n",
      "Epoch 17/1000\n",
      "4292/4292 [==============================] - 733s 171ms/step - loss: 0.6291 - accuracy: 0.7477 - val_loss: 0.5701 - val_accuracy: 0.7738\n",
      "Epoch 18/1000\n",
      "4292/4292 [==============================] - 735s 171ms/step - loss: 0.6260 - accuracy: 0.7485 - val_loss: 0.5738 - val_accuracy: 0.7780\n",
      "Epoch 19/1000\n",
      "4292/4292 [==============================] - 742s 173ms/step - loss: 0.6247 - accuracy: 0.7493 - val_loss: 0.5658 - val_accuracy: 0.7800\n",
      "Epoch 20/1000\n",
      "4292/4292 [==============================] - 765s 178ms/step - loss: 0.6231 - accuracy: 0.7510 - val_loss: 0.5656 - val_accuracy: 0.7779\n",
      "Epoch 21/1000\n",
      "4292/4292 [==============================] - 739s 172ms/step - loss: 0.6200 - accuracy: 0.7518 - val_loss: 0.5670 - val_accuracy: 0.7768\n",
      "Epoch 22/1000\n",
      "4292/4292 [==============================] - 743s 173ms/step - loss: 0.6181 - accuracy: 0.7528 - val_loss: 0.5704 - val_accuracy: 0.7767\n",
      "Epoch 23/1000\n",
      "4292/4292 [==============================] - 765s 178ms/step - loss: 0.6173 - accuracy: 0.7533 - val_loss: 0.5684 - val_accuracy: 0.7780\n",
      "Epoch 24/1000\n",
      "4292/4292 [==============================] - 744s 173ms/step - loss: 0.6160 - accuracy: 0.7537 - val_loss: 0.5765 - val_accuracy: 0.7702\n",
      "Epoch 25/1000\n",
      "4292/4292 [==============================] - 743s 173ms/step - loss: 0.6147 - accuracy: 0.7556 - val_loss: 0.5597 - val_accuracy: 0.7785\n",
      "Epoch 26/1000\n",
      "4292/4292 [==============================] - 728s 170ms/step - loss: 0.6130 - accuracy: 0.7556 - val_loss: 0.5614 - val_accuracy: 0.7799\n",
      "Epoch 27/1000\n",
      "4292/4292 [==============================] - 731s 170ms/step - loss: 0.6116 - accuracy: 0.7568 - val_loss: 0.5607 - val_accuracy: 0.7819\n",
      "Epoch 28/1000\n",
      "4292/4292 [==============================] - 743s 173ms/step - loss: 0.6102 - accuracy: 0.7571 - val_loss: 0.5613 - val_accuracy: 0.7789\n",
      "Epoch 29/1000\n",
      "4292/4292 [==============================] - 777s 181ms/step - loss: 0.6091 - accuracy: 0.7576 - val_loss: 0.5602 - val_accuracy: 0.7825\n",
      "Epoch 30/1000\n",
      "4292/4292 [==============================] - 764s 178ms/step - loss: 0.6082 - accuracy: 0.7582 - val_loss: 0.5621 - val_accuracy: 0.7821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f472b3d2730>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model using the train data\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n",
    "model.fit([array(trainData[0]), array(trainData[1])], array(trainData[2]), batch_size=batchSize, epochs=epochs, callbacks=[callback], validation_data=[[array(validationData[0]), array(validationData[1])], array(validationData[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 3s 86ms/step - loss: 0.5769 - accuracy: 0.7730\n",
      "Loss =  0.5768946409225464\n",
      "Acc =  0.7730048894882202\n"
     ]
    }
   ],
   "source": [
    "# Evaluating accuracy on the trained model\n",
    "\n",
    "loss, acc = model.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
    "print('Loss = ', loss)\n",
    "print('Acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../Models/LSTM_snli.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 4s 78ms/step - loss: 0.5769 - accuracy: 0.7730\n",
      "0.7730048894882202\n"
     ]
    }
   ],
   "source": [
    "ccc = keras.models.load_model('../../Models/LSTM_snli.h5')\n",
    "l, a = ccc.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus observe the following results for the simple LSTM model, for the SNLI dataset:\n",
    "\n",
    "- Training Accuracy: 75.82%\n",
    "- Validation Accuracy: 78.21%\n",
    "- Test Accuracy: 77.30%"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
