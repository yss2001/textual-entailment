{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shared LSTM Model - SICK Dataset\n",
        "\n",
        "The following notebook contains the implementation of the shared LSTM model for the SICK dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkfB55GS6ZnP",
        "outputId": "9e54925b-f09e-40ff-f9c8-c52821fbd86f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/aakashj2412/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "\n",
        "import re\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import L2\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Dropout, Input, LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from gensim import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4DjPgbVd6ZnT"
      },
      "outputs": [],
      "source": [
        "# Helper function that cleans the input data and enumerates the labels\n",
        "\n",
        "def extract(s):\n",
        "    s = re.sub('\\\\(', '', s)\n",
        "    s = re.sub('\\\\)', '', s)\n",
        "    s = re.sub('\\\\s{2,}', ' ', s)\n",
        "    return s.strip()\n",
        "\n",
        "labels = {'ENTAILMENT': 0, 'CONTRADICTION': 1, 'NEUTRAL': 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8jwY84sY6ZnU"
      },
      "outputs": [],
      "source": [
        "# Function that reads data and parses data from file\n",
        "\n",
        "datasetPath = '../../Datasets/SICK/SICK.txt'\n",
        "\n",
        "with open(datasetPath, 'r') as f:\n",
        "    Rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
        "\n",
        "trainRows = [row for row in Rows if row[11]=='TRAIN\\n']\n",
        "testRows = [row for row in Rows if row[11]=='TEST\\n']\n",
        "validationRows = [row for row in Rows if row[11]=='TRIAL\\n']\n",
        "\n",
        "trainPremises = [extract(row[1]) for row in trainRows if row[3] in labels]\n",
        "trainHypotheses = [extract(row[2]) for row in trainRows if row[3] in labels]\n",
        "trainLabels = [labels[row[3]] for row in trainRows if row[3] in labels]\n",
        "\n",
        "trainData = [trainPremises, trainHypotheses, trainLabels]\n",
        "\n",
        "testPremises = [extract(row[1]) for row in testRows if row[3] in labels]\n",
        "testHypotheses = [extract(row[2]) for row in testRows if row[3] in labels]\n",
        "testLabels = [labels[row[3]] for row in testRows if row[3] in labels]\n",
        "\n",
        "testData = [testPremises, testHypotheses, testLabels]\n",
        "\n",
        "validationPremises = [extract(row[1]) for row in validationRows if row[3] in labels]\n",
        "validationHypotheses = [extract(row[2]) for row in validationRows if row[3] in labels]\n",
        "validationLabels = [labels[row[3]] for row in validationRows if row[3] in labels]\n",
        "\n",
        "validationData = [validationPremises, validationHypotheses, validationLabels]\n",
        "\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cIVwthL46ZnV"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "maxLen = 32\n",
        "epochs = 1000\n",
        "batchSize = 512\n",
        "gloveDimension = 300\n",
        "hiddenDimension = 100\n",
        "regularization = 4e-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n0X9TGPc6ZnW"
      },
      "outputs": [],
      "source": [
        "# Tokenizer to generate the vocabulary of the system\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(trainData[0] + trainData[1])\n",
        "vocabSize = len(tokenizer.word_index)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DLqheMvs6ZnW"
      },
      "outputs": [],
      "source": [
        "# Convert the train data to sequences as per the vocabulary\n",
        "trainData[0] = tokenizer.texts_to_sequences(trainData[0])\n",
        "trainData[1] = tokenizer.texts_to_sequences(trainData[1])\n",
        "\n",
        "# Pad or trim all generated sequences to the same max sentence length\n",
        "trainData[0] = pad_sequences(trainData[0], maxLen, padding='post')\n",
        "trainData[1] = pad_sequences(trainData[1], maxLen, padding='post')\n",
        "\n",
        "# Transform the labels to one-hot encoding\n",
        "trainData[2] = tf.keras.utils.to_categorical(trainData[2], num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6QeVNUhh6ZnW"
      },
      "outputs": [],
      "source": [
        "# Convert the test data to sequences as per the vocabulary\n",
        "testData[0] = tokenizer.texts_to_sequences(testData[0])\n",
        "testData[1] = tokenizer.texts_to_sequences(testData[1])\n",
        "\n",
        "# Pad or trim all generated sequences to the same max sentence length\n",
        "testData[0] = pad_sequences(testData[0], maxLen, padding='post')\n",
        "testData[1] = pad_sequences(testData[1], maxLen, padding='post')\n",
        "\n",
        "# Transform the labels to one-hot encoding\n",
        "testData[2] = tf.keras.utils.to_categorical(testData[2], num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z6vN69Pj6ZnX"
      },
      "outputs": [],
      "source": [
        "# Convert the validation data to sequences as per the vocabulary\n",
        "validationData[0] = tokenizer.texts_to_sequences(validationData[0])\n",
        "validationData[1] = tokenizer.texts_to_sequences(validationData[1])\n",
        "\n",
        "# Pad or trim all generated sequences to the same max sentence length\n",
        "validationData[0] = pad_sequences(validationData[0], maxLen, padding='post')\n",
        "validationData[1] = pad_sequences(validationData[1], maxLen, padding='post')\n",
        "\n",
        "# Transform the labels to one-hot encoding\n",
        "validationData[2] = tf.keras.utils.to_categorical(validationData[2], num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the embeddings dictionary from Google News word2vec\n",
        "\n",
        "word2vecpath = \"../../Datasets/Word2Vec/\"\n",
        "embeddingsDict = models.KeyedVectors.load_word2vec_format(f'{word2vecpath}GoogleNews-vectors-negative300.bin', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNWQ50Fq6ZnY"
      },
      "outputs": [],
      "source": [
        "# Alternative GloVe embeddings\n",
        "\n",
        "'''\n",
        "embeddingsDict = dict()\n",
        "glove = open(r'/content/drive/MyDrive/glove.840B.300d.txt', encoding='utf8')\n",
        "\n",
        "for line in glove:\n",
        "    records = line.split()\n",
        "    word = ''.join(records[:-300])\n",
        "    vectorDimensions = asarray(records[-300:], dtype='float32')\n",
        "    embeddingsDict[word] = vectorDimensions\n",
        "\n",
        "glove.close()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dwPl7uUN6ZnY"
      },
      "outputs": [],
      "source": [
        "# Iterate through the embeddings and store only those that are present in our vocabulary\n",
        "'''\n",
        "embeddingsMat = zeros((vocabSize, gloveDimension))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    vec = embeddingsDict.get(word)\n",
        "    if vec is not None:\n",
        "        embeddingsMat[index] = vec\n",
        "'''\n",
        "\n",
        "embeddingsMat = zeros((vocabSize, gloveDimension))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index % 2500 == 0:\n",
        "        print(index)\n",
        "    if word in embeddingsDict.index_to_key:\n",
        "        embeddingsMat[index] = embeddingsDict[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZXbOSpB56ZnY"
      },
      "outputs": [],
      "source": [
        "# Define the embedding layer for our baseline RNN model\n",
        "embed = Embedding(vocabSize, gloveDimension, weights=[embeddingsMat], input_length=maxLen, trainable=False)\n",
        "\n",
        "# As Premise and Hypothesis are distinct and are to be inputted separately, define two inputs and embed\n",
        "premise = Input(shape=(maxLen,), dtype='int32')\n",
        "hypothesis = Input(shape=(maxLen,), dtype='int32')\n",
        "\n",
        "premInput = embed(premise)\n",
        "hypoInput = embed(hypothesis)\n",
        "\n",
        "convert = Dense(hiddenDimension, activation='tanh', input_shape=(gloveDimension,))\n",
        "\n",
        "premInput = convert(premInput)\n",
        "hypoInput = convert(hypoInput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pJ6YggVR6ZnZ"
      },
      "outputs": [],
      "source": [
        "# Once the sentence embeddings have been generated, generate a matrix of dimensions maxLen X gloveDimension\n",
        "# On adding maxLen, we get a single embedding vector of length gloveDimension\n",
        "\n",
        "#rnn = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=hiddenDimension)\n",
        "#rnn = LSTM(hiddenDimension, dropout=0.2, recurrent_dropout=0.2)\n",
        "\n",
        "rnnH = LSTM(hiddenDimension, dropout=0.2, return_state=True)\n",
        "rnnP = LSTM(hiddenDimension, dropout=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8fZbzv_-6ZnZ"
      },
      "outputs": [],
      "source": [
        "# Apply batch normalization to the two input embeddings separately\n",
        "\n",
        "premInput, sH, sC = rnnH(premInput)\n",
        "hypoInput = rnnP(hypoInput, initial_state=[sH, sC])\n",
        "premInput = tf.keras.layers.BatchNormalization()(premInput)\n",
        "result = tf.keras.layers.BatchNormalization()(hypoInput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gwe0F0N-6ZnZ"
      },
      "outputs": [],
      "source": [
        "# Joint is a concatenated embeddings layer, generated from the premise and hypothesis inputs\n",
        "# Dilution of probability 0.2, to assist in regularization\n",
        "#joint = keras.layers.concatenate([premInput, hypoInput])\n",
        "result = Dropout(0.2)(result)\n",
        "'''\n",
        "for i in range(3):\n",
        "    result = Dense(2*hiddenDimension, activation='tanh', kernel_regularizer=L2(regularization))(result)\n",
        "    result = Dropout(0.2)(result)\n",
        "    result = tf.keras.layers.BatchNormalization()(result)\n",
        "'''\n",
        "\n",
        "# 3 layers of the TanH activation function, along with L2 regularization.\n",
        "# The final decision is based on the Softmax function\n",
        "\n",
        "result = Dense(hiddenDimension, activation='tanh', kernel_regularizer=L2(regularization))(result)\n",
        "pred = Dense(3, activation='softmax')(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Q4XPOMBB6Zna"
      },
      "outputs": [],
      "source": [
        "# Defining the final models input and output format, as well as compilation parameters\n",
        "\n",
        "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "9/9 [==============================] - 8s 469ms/step - loss: 0.2022 - accuracy: 0.9311 - val_loss: 2.1165 - val_accuracy: 0.5596\n",
            "Epoch 2/1000\n",
            "9/9 [==============================] - 3s 380ms/step - loss: 0.0664 - accuracy: 0.9754 - val_loss: 2.3739 - val_accuracy: 0.5556\n",
            "Epoch 3/1000\n",
            "9/9 [==============================] - 3s 381ms/step - loss: 0.0967 - accuracy: 0.9664 - val_loss: 2.3299 - val_accuracy: 0.5859\n",
            "Epoch 4/1000\n",
            "9/9 [==============================] - 3s 376ms/step - loss: 0.0840 - accuracy: 0.9691 - val_loss: 2.2266 - val_accuracy: 0.6020\n",
            "Epoch 5/1000\n",
            "9/9 [==============================] - 3s 382ms/step - loss: 0.0799 - accuracy: 0.9703 - val_loss: 2.2295 - val_accuracy: 0.5939\n",
            "Epoch 6/1000\n",
            "9/9 [==============================] - 3s 377ms/step - loss: 0.0842 - accuracy: 0.9676 - val_loss: 2.4253 - val_accuracy: 0.5879\n",
            "Epoch 7/1000\n",
            "9/9 [==============================] - 3s 371ms/step - loss: 0.0779 - accuracy: 0.9682 - val_loss: 2.3486 - val_accuracy: 0.5657\n",
            "Epoch 8/1000\n",
            "9/9 [==============================] - 3s 379ms/step - loss: 0.0782 - accuracy: 0.9700 - val_loss: 2.3461 - val_accuracy: 0.5778\n",
            "Epoch 9/1000\n",
            "9/9 [==============================] - 3s 375ms/step - loss: 0.0676 - accuracy: 0.9770 - val_loss: 2.4794 - val_accuracy: 0.5818\n",
            "Epoch 10/1000\n",
            "9/9 [==============================] - 4s 397ms/step - loss: 0.0830 - accuracy: 0.9694 - val_loss: 2.3164 - val_accuracy: 0.5717\n",
            "Epoch 11/1000\n",
            "9/9 [==============================] - 3s 380ms/step - loss: 0.0652 - accuracy: 0.9761 - val_loss: 2.4650 - val_accuracy: 0.6061\n",
            "Epoch 12/1000\n",
            "9/9 [==============================] - 3s 376ms/step - loss: 0.0681 - accuracy: 0.9752 - val_loss: 2.5387 - val_accuracy: 0.5636\n",
            "Epoch 13/1000\n",
            "9/9 [==============================] - 3s 376ms/step - loss: 0.0732 - accuracy: 0.9750 - val_loss: 2.5427 - val_accuracy: 0.5758\n",
            "Epoch 14/1000\n",
            "9/9 [==============================] - 3s 381ms/step - loss: 0.0678 - accuracy: 0.9781 - val_loss: 2.4429 - val_accuracy: 0.5717\n",
            "Epoch 15/1000\n",
            "9/9 [==============================] - 3s 376ms/step - loss: 0.0921 - accuracy: 0.9667 - val_loss: 2.4215 - val_accuracy: 0.5697\n",
            "Epoch 16/1000\n",
            "9/9 [==============================] - 3s 384ms/step - loss: 0.0691 - accuracy: 0.9750 - val_loss: 2.4238 - val_accuracy: 0.5515\n",
            "Epoch 17/1000\n",
            "9/9 [==============================] - 3s 381ms/step - loss: 0.0608 - accuracy: 0.9766 - val_loss: 2.2871 - val_accuracy: 0.5737\n",
            "Epoch 18/1000\n",
            "9/9 [==============================] - 3s 375ms/step - loss: 0.0724 - accuracy: 0.9718 - val_loss: 2.2993 - val_accuracy: 0.5838\n",
            "Epoch 19/1000\n",
            "9/9 [==============================] - 3s 381ms/step - loss: 0.0642 - accuracy: 0.9784 - val_loss: 2.4948 - val_accuracy: 0.5657\n",
            "Epoch 20/1000\n",
            "9/9 [==============================] - 3s 377ms/step - loss: 0.0744 - accuracy: 0.9745 - val_loss: 2.4771 - val_accuracy: 0.5556\n",
            "Epoch 21/1000\n",
            "9/9 [==============================] - 4s 392ms/step - loss: 0.0621 - accuracy: 0.9779 - val_loss: 2.4404 - val_accuracy: 0.5717\n",
            "Epoch 22/1000\n",
            "9/9 [==============================] - 3s 376ms/step - loss: 0.0755 - accuracy: 0.9736 - val_loss: 2.2658 - val_accuracy: 0.5980\n",
            "Epoch 23/1000\n",
            "9/9 [==============================] - 3s 376ms/step - loss: 0.0664 - accuracy: 0.9768 - val_loss: 2.2365 - val_accuracy: 0.6182\n",
            "Epoch 24/1000\n",
            "9/9 [==============================] - 3s 386ms/step - loss: 0.0668 - accuracy: 0.9748 - val_loss: 2.3280 - val_accuracy: 0.5859\n",
            "Epoch 25/1000\n",
            "9/9 [==============================] - 4s 405ms/step - loss: 0.0711 - accuracy: 0.9739 - val_loss: 2.3073 - val_accuracy: 0.5919\n",
            "Epoch 26/1000\n",
            "9/9 [==============================] - 3s 372ms/step - loss: 0.0621 - accuracy: 0.9790 - val_loss: 2.2636 - val_accuracy: 0.5919\n",
            "Epoch 27/1000\n",
            "9/9 [==============================] - 3s 380ms/step - loss: 0.0751 - accuracy: 0.9727 - val_loss: 2.3345 - val_accuracy: 0.5899\n",
            "Epoch 28/1000\n",
            "9/9 [==============================] - 3s 375ms/step - loss: 0.0628 - accuracy: 0.9775 - val_loss: 2.4369 - val_accuracy: 0.5879\n",
            "Epoch 29/1000\n",
            "9/9 [==============================] - 3s 370ms/step - loss: 0.0729 - accuracy: 0.9741 - val_loss: 2.2309 - val_accuracy: 0.6182\n",
            "Epoch 30/1000\n",
            "9/9 [==============================] - 3s 391ms/step - loss: 0.0723 - accuracy: 0.9712 - val_loss: 2.3267 - val_accuracy: 0.5737\n",
            "Epoch 31/1000\n",
            "9/9 [==============================] - 3s 382ms/step - loss: 0.0623 - accuracy: 0.9806 - val_loss: 2.4544 - val_accuracy: 0.5717\n",
            "Epoch 32/1000\n",
            "9/9 [==============================] - 3s 384ms/step - loss: 0.0522 - accuracy: 0.9827 - val_loss: 2.4233 - val_accuracy: 0.5960\n",
            "Epoch 33/1000\n",
            "9/9 [==============================] - 3s 378ms/step - loss: 0.0683 - accuracy: 0.9745 - val_loss: 2.1662 - val_accuracy: 0.5798\n",
            "Epoch 34/1000\n",
            "9/9 [==============================] - 3s 376ms/step - loss: 0.0584 - accuracy: 0.9775 - val_loss: 2.5462 - val_accuracy: 0.5737\n",
            "Epoch 35/1000\n",
            "9/9 [==============================] - 3s 383ms/step - loss: 0.0706 - accuracy: 0.9752 - val_loss: 2.2006 - val_accuracy: 0.5919\n",
            "Epoch 36/1000\n",
            "9/9 [==============================] - 3s 384ms/step - loss: 0.0522 - accuracy: 0.9813 - val_loss: 2.3222 - val_accuracy: 0.5859\n",
            "Epoch 37/1000\n",
            "9/9 [==============================] - 3s 376ms/step - loss: 0.0551 - accuracy: 0.9757 - val_loss: 2.4518 - val_accuracy: 0.5899\n",
            "Epoch 38/1000\n",
            "9/9 [==============================] - 4s 401ms/step - loss: 0.0664 - accuracy: 0.9754 - val_loss: 2.3490 - val_accuracy: 0.6040\n",
            "Epoch 39/1000\n",
            "9/9 [==============================] - 3s 373ms/step - loss: 0.0544 - accuracy: 0.9788 - val_loss: 2.5176 - val_accuracy: 0.5838\n",
            "Epoch 40/1000\n",
            "9/9 [==============================] - 3s 379ms/step - loss: 0.0717 - accuracy: 0.9741 - val_loss: 2.4042 - val_accuracy: 0.5778\n",
            "Epoch 41/1000\n",
            "9/9 [==============================] - 3s 389ms/step - loss: 0.0498 - accuracy: 0.9804 - val_loss: 2.5164 - val_accuracy: 0.6162\n",
            "Epoch 42/1000\n",
            "9/9 [==============================] - 3s 377ms/step - loss: 0.0642 - accuracy: 0.9763 - val_loss: 2.4235 - val_accuracy: 0.5859\n",
            "Epoch 43/1000\n",
            "9/9 [==============================] - 3s 382ms/step - loss: 0.0489 - accuracy: 0.9822 - val_loss: 2.3712 - val_accuracy: 0.5859\n",
            "Epoch 44/1000\n",
            "9/9 [==============================] - 3s 385ms/step - loss: 0.0496 - accuracy: 0.9820 - val_loss: 2.4308 - val_accuracy: 0.5657\n",
            "Epoch 45/1000\n",
            "9/9 [==============================] - 3s 374ms/step - loss: 0.0716 - accuracy: 0.9748 - val_loss: 2.3341 - val_accuracy: 0.5717\n",
            "Epoch 46/1000\n",
            "9/9 [==============================] - 3s 383ms/step - loss: 0.0525 - accuracy: 0.9797 - val_loss: 2.4558 - val_accuracy: 0.6020\n",
            "Epoch 47/1000\n",
            "9/9 [==============================] - 3s 386ms/step - loss: 0.0536 - accuracy: 0.9793 - val_loss: 2.4915 - val_accuracy: 0.6040\n",
            "Epoch 48/1000\n",
            "9/9 [==============================] - 3s 384ms/step - loss: 0.0646 - accuracy: 0.9739 - val_loss: 2.3907 - val_accuracy: 0.5939\n",
            "Epoch 49/1000\n",
            "9/9 [==============================] - 4s 391ms/step - loss: 0.0405 - accuracy: 0.9867 - val_loss: 2.5272 - val_accuracy: 0.5899\n",
            "Epoch 50/1000\n",
            "9/9 [==============================] - 3s 386ms/step - loss: 0.0760 - accuracy: 0.9750 - val_loss: 2.4635 - val_accuracy: 0.5758\n",
            "Epoch 51/1000\n",
            "9/9 [==============================] - 3s 377ms/step - loss: 0.0386 - accuracy: 0.9860 - val_loss: 2.4248 - val_accuracy: 0.5879\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbfd2024610>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fitting the model using the train data\n",
        "\n",
        "callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=50)\n",
        "mcCallback = ModelCheckpoint('./2LSTM_sick.h5', monitor='val_loss')\n",
        "model.fit([array(trainData[0]), array(trainData[1])], array(trainData[2]), batch_size=batchSize, epochs=epochs, callbacks=[callback, mcCallback], validation_data=[[array(validationData[0]), array(validationData[1])], array(validationData[2])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('../../Models/2LSTM_sick.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.models.load_model('../../Models/2LSTM_sick.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "C58V0Ch76Znb",
        "outputId": "7f877f90-f67f-4eaa-a940-70268bd9b5d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 1s 68ms/step - loss: 2.2850 - accuracy: 0.6058\n",
            "Loss =  2.2849695682525635\n",
            "Acc =  0.6057888269424438\n"
          ]
        }
      ],
      "source": [
        "# Evaluating accuracy on the trained model\n",
        "\n",
        "loss, acc = model.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
        "print('Loss = ', loss)\n",
        "print('Acc = ', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can thus observe the following results for the shared LSTM model, for the SICK dataset:\n",
        "\n",
        "- Training Accuracy: 98.60%\n",
        "- Validation Accuracy: 58.79%\n",
        "- Test Accuracy: 60.57%\n",
        "\n",
        "We can see, based on the training and test data, that the model has slightly overfit on the data, as in the other SICK models as well. This can be controlled to a certain degree by optimizing the patience value for early stopping and other hyperparameters. However, as we observe in the SciTail dataset, the amount of data of the dataset isn't sufficient for the shared LSTM model, even more so in the case of SICK, as it is much smaller than SciTail."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of summationBowman.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
