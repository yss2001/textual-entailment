{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shared LSTM Model - SciTail Dataset\n",
        "\n",
        "The following notebook contains the implementation of the shared LSTM model for the SciTail dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkfB55GS6ZnP",
        "outputId": "9e54925b-f09e-40ff-f9c8-c52821fbd86f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/aakashj2412/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "\n",
        "import re\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.models import Model\n",
        "from keras.regularizers import L2\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Dropout, Input, LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from gensim import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4DjPgbVd6ZnT"
      },
      "outputs": [],
      "source": [
        "# Helper function that cleans the input data and enumerates the labels\n",
        "\n",
        "def extract(s):\n",
        "    s = re.sub('\\\\(', '', s)\n",
        "    s = re.sub('\\\\)', '', s)\n",
        "    s = re.sub('\\\\s{2,}', ' ', s)\n",
        "    return s.strip()\n",
        "\n",
        "labels = {'entails\\n': 0, 'contradiction': 1, 'neutral\\n': 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8jwY84sY6ZnU"
      },
      "outputs": [],
      "source": [
        "# Function that reads data and parses data from file\n",
        "\n",
        "def readFileData(filePath):\n",
        "    with open(filePath, 'r') as f:\n",
        "        inputRows = [row.split('\\t') for row in f.readlines()[1:]]\n",
        "\n",
        "    inputPremises = [row[0] for row in inputRows if row[2] in labels]\n",
        "    inputHypotheses = [row[1] for row in inputRows if row[2] in labels]\n",
        "    inputLabels = [labels[row[2]] for row in inputRows if row[2] in labels]\n",
        "    f.close()\n",
        "\n",
        "    return [inputPremises, inputHypotheses, inputLabels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "F4nBYHiz6ZnV"
      },
      "outputs": [],
      "source": [
        "# Reading train and test data\n",
        "\n",
        "datasetPath = \"../../Datasets/SciTail/\"\n",
        "trainData = readFileData(f'{datasetPath}scitail_1.0_train.tsv')\n",
        "testData = readFileData(f'{datasetPath}scitail_1.0_test.tsv')\n",
        "validationData = readFileData(f'{datasetPath}scitail_1.0_dev.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cIVwthL46ZnV"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "maxLen = 32\n",
        "epochs = 1000\n",
        "batchSize = 512\n",
        "gloveDimension = 300\n",
        "hiddenDimension = 100\n",
        "regularization = 4e-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n0X9TGPc6ZnW"
      },
      "outputs": [],
      "source": [
        "# Tokenizer to generate the vocabulary of the system\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(trainData[0] + trainData[1])\n",
        "vocabSize = len(tokenizer.word_index)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DLqheMvs6ZnW"
      },
      "outputs": [],
      "source": [
        "# Convert the train data to sequences as per the vocabulary\n",
        "trainData[0] = tokenizer.texts_to_sequences(trainData[0])\n",
        "trainData[1] = tokenizer.texts_to_sequences(trainData[1])\n",
        "\n",
        "# Pad or trim all generated sequences to the same max sentence length\n",
        "trainData[0] = pad_sequences(trainData[0], maxLen, padding='post')\n",
        "trainData[1] = pad_sequences(trainData[1], maxLen, padding='post')\n",
        "\n",
        "# Transform the labels to one-hot encoding\n",
        "trainData[2] = tf.keras.utils.to_categorical(trainData[2], num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6QeVNUhh6ZnW"
      },
      "outputs": [],
      "source": [
        "# Convert the test data to sequences as per the vocabulary\n",
        "testData[0] = tokenizer.texts_to_sequences(testData[0])\n",
        "testData[1] = tokenizer.texts_to_sequences(testData[1])\n",
        "\n",
        "# Pad or trim all generated sequences to the same max sentence length\n",
        "testData[0] = pad_sequences(testData[0], maxLen, padding='post')\n",
        "testData[1] = pad_sequences(testData[1], maxLen, padding='post')\n",
        "\n",
        "# Transform the labels to one-hot encoding\n",
        "testData[2] = tf.keras.utils.to_categorical(testData[2], num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Z6vN69Pj6ZnX"
      },
      "outputs": [],
      "source": [
        "# Convert the validation data to sequences as per the vocabulary\n",
        "validationData[0] = tokenizer.texts_to_sequences(validationData[0])\n",
        "validationData[1] = tokenizer.texts_to_sequences(validationData[1])\n",
        "\n",
        "# Pad or trim all generated sequences to the same max sentence length\n",
        "validationData[0] = pad_sequences(validationData[0], maxLen, padding='post')\n",
        "validationData[1] = pad_sequences(validationData[1], maxLen, padding='post')\n",
        "\n",
        "# Transform the labels to one-hot encoding\n",
        "validationData[2] = tf.keras.utils.to_categorical(validationData[2], num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the embeddings dictionary from Google News word2vec\n",
        "\n",
        "word2vecpath = \"../../Datasets/Word2Vec/\"\n",
        "embeddingsDict = models.KeyedVectors.load_word2vec_format(f'{word2vecpath}GoogleNews-vectors-negative300.bin', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNWQ50Fq6ZnY"
      },
      "outputs": [],
      "source": [
        "# Alternative GloVe embeddings\n",
        "\n",
        "'''\n",
        "embeddingsDict = dict()\n",
        "glove = open(r'/content/drive/MyDrive/glove.840B.300d.txt', encoding='utf8')\n",
        "\n",
        "for line in glove:\n",
        "    records = line.split()\n",
        "    word = ''.join(records[:-300])\n",
        "    vectorDimensions = asarray(records[-300:], dtype='float32')\n",
        "    embeddingsDict[word] = vectorDimensions\n",
        "\n",
        "glove.close()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dwPl7uUN6ZnY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2500\n",
            "5000\n",
            "7500\n",
            "10000\n",
            "12500\n",
            "15000\n",
            "17500\n",
            "20000\n"
          ]
        }
      ],
      "source": [
        "# Iterate through the embeddings and store only those that are present in our vocabulary\n",
        "'''\n",
        "embeddingsMat = zeros((vocabSize, gloveDimension))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    vec = embeddingsDict.get(word)\n",
        "    if vec is not None:\n",
        "        embeddingsMat[index] = vec\n",
        "'''\n",
        "\n",
        "embeddingsMat = zeros((vocabSize, gloveDimension))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index % 2500 == 0:\n",
        "        print(index)\n",
        "    if word in embeddingsDict.index_to_key:\n",
        "        embeddingsMat[index] = embeddingsDict[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZXbOSpB56ZnY"
      },
      "outputs": [],
      "source": [
        "# Define the embedding layer for our baseline RNN model\n",
        "embed = Embedding(vocabSize, gloveDimension, weights=[embeddingsMat], input_length=maxLen, trainable=False)\n",
        "\n",
        "# As Premise and Hypothesis are distinct and are to be inputted separately, define two inputs and embed\n",
        "premise = Input(shape=(maxLen,), dtype='int32')\n",
        "hypothesis = Input(shape=(maxLen,), dtype='int32')\n",
        "\n",
        "premInput = embed(premise)\n",
        "hypoInput = embed(hypothesis)\n",
        "\n",
        "convert = Dense(hiddenDimension, activation='tanh', input_shape=(gloveDimension,))\n",
        "\n",
        "premInput = convert(premInput)\n",
        "hypoInput = convert(hypoInput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pJ6YggVR6ZnZ"
      },
      "outputs": [],
      "source": [
        "# Once the sentence embeddings have been generated, generate a matrix of dimensions maxLen X gloveDimension\n",
        "# On adding maxLen, we get a single embedding vector of length gloveDimension\n",
        "\n",
        "#rnn = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=hiddenDimension)\n",
        "#rnn = LSTM(hiddenDimension, dropout=0.2, recurrent_dropout=0.2)\n",
        "\n",
        "rnnH = LSTM(hiddenDimension, dropout=0.2, return_state=True)\n",
        "rnnP = LSTM(hiddenDimension, dropout=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8fZbzv_-6ZnZ"
      },
      "outputs": [],
      "source": [
        "# Apply batch normalization to the two input embeddings separately\n",
        "\n",
        "premInput, sH, sC = rnnH(premInput)\n",
        "hypoInput = rnnP(hypoInput, initial_state=[sH, sC])\n",
        "premInput = tf.keras.layers.BatchNormalization()(premInput)\n",
        "result = tf.keras.layers.BatchNormalization()(hypoInput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gwe0F0N-6ZnZ"
      },
      "outputs": [],
      "source": [
        "# Joint is a concatenated embeddings layer, generated from the premise and hypothesis inputs\n",
        "# Dilution of probability 0.2, to assist in regularization\n",
        "#joint = keras.layers.concatenate([premInput, hypoInput])\n",
        "result = Dropout(0.2)(result)\n",
        "'''\n",
        "for i in range(3):\n",
        "    result = Dense(2*hiddenDimension, activation='tanh', kernel_regularizer=L2(regularization))(result)\n",
        "    result = Dropout(0.2)(result)\n",
        "    result = tf.keras.layers.BatchNormalization()(result)\n",
        "'''\n",
        "\n",
        "# 3 layers of the TanH activation function, along with L2 regularization.\n",
        "# The final decision is based on the Softmax function\n",
        "\n",
        "result = Dense(hiddenDimension, activation='tanh', kernel_regularizer=L2(regularization))(result)\n",
        "pred = Dense(3, activation='softmax')(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Q4XPOMBB6Zna"
      },
      "outputs": [],
      "source": [
        "# Defining the final models input and output format, as well as compilation parameters\n",
        "\n",
        "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 16s 331ms/step - loss: 0.1744 - accuracy: 0.9322 - val_loss: 1.0275 - val_accuracy: 0.6930\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 17s 368ms/step - loss: 0.1710 - accuracy: 0.9324 - val_loss: 0.9881 - val_accuracy: 0.6470\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 16s 349ms/step - loss: 0.1669 - accuracy: 0.9341 - val_loss: 1.0195 - val_accuracy: 0.6616\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 17s 369ms/step - loss: 0.1596 - accuracy: 0.9360 - val_loss: 0.7334 - val_accuracy: 0.6807\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 17s 370ms/step - loss: 0.1570 - accuracy: 0.9385 - val_loss: 1.2739 - val_accuracy: 0.6447\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 17s 352ms/step - loss: 0.1540 - accuracy: 0.9408 - val_loss: 0.9913 - val_accuracy: 0.6884\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 17s 352ms/step - loss: 0.1509 - accuracy: 0.9401 - val_loss: 1.2282 - val_accuracy: 0.6631\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 17s 358ms/step - loss: 0.1481 - accuracy: 0.9416 - val_loss: 1.1089 - val_accuracy: 0.6477\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 17s 372ms/step - loss: 0.1448 - accuracy: 0.9429 - val_loss: 0.9238 - val_accuracy: 0.6853\n",
            "Epoch 10/1000\n",
            "47/47 [==============================] - 18s 382ms/step - loss: 0.1370 - accuracy: 0.9463 - val_loss: 0.7597 - val_accuracy: 0.6539\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 17s 368ms/step - loss: 0.1388 - accuracy: 0.9467 - val_loss: 0.8618 - val_accuracy: 0.6639\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 16s 343ms/step - loss: 0.1395 - accuracy: 0.9460 - val_loss: 1.2525 - val_accuracy: 0.6685\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 16s 344ms/step - loss: 0.1332 - accuracy: 0.9471 - val_loss: 1.3766 - val_accuracy: 0.6662\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 17s 351ms/step - loss: 0.1346 - accuracy: 0.9487 - val_loss: 1.1670 - val_accuracy: 0.6692\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 16s 349ms/step - loss: 0.1255 - accuracy: 0.9501 - val_loss: 1.5651 - val_accuracy: 0.6608\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 18s 373ms/step - loss: 0.1214 - accuracy: 0.9521 - val_loss: 1.0824 - val_accuracy: 0.6424\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 18s 373ms/step - loss: 0.1179 - accuracy: 0.9538 - val_loss: 0.7421 - val_accuracy: 0.6278\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 17s 370ms/step - loss: 0.1178 - accuracy: 0.9537 - val_loss: 1.1889 - val_accuracy: 0.6316\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 18s 373ms/step - loss: 0.1172 - accuracy: 0.9536 - val_loss: 1.1013 - val_accuracy: 0.6946\n",
            "Epoch 20/1000\n",
            "47/47 [==============================] - 18s 372ms/step - loss: 0.1108 - accuracy: 0.9568 - val_loss: 1.1799 - val_accuracy: 0.6761\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 17s 371ms/step - loss: 0.1069 - accuracy: 0.9597 - val_loss: 1.1603 - val_accuracy: 0.6370\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 18s 373ms/step - loss: 0.1102 - accuracy: 0.9580 - val_loss: 1.2893 - val_accuracy: 0.6723\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 18s 376ms/step - loss: 0.1082 - accuracy: 0.9586 - val_loss: 0.9836 - val_accuracy: 0.6577\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 18s 372ms/step - loss: 0.1066 - accuracy: 0.9586 - val_loss: 1.1883 - val_accuracy: 0.6853\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 18s 374ms/step - loss: 0.0980 - accuracy: 0.9626 - val_loss: 1.3975 - val_accuracy: 0.6562\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 18s 379ms/step - loss: 0.0985 - accuracy: 0.9627 - val_loss: 1.4393 - val_accuracy: 0.6846\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 17s 367ms/step - loss: 0.0945 - accuracy: 0.9633 - val_loss: 1.3808 - val_accuracy: 0.6562\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 17s 362ms/step - loss: 0.0935 - accuracy: 0.9640 - val_loss: 1.3738 - val_accuracy: 0.6539\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 17s 368ms/step - loss: 0.0897 - accuracy: 0.9666 - val_loss: 1.2292 - val_accuracy: 0.7007\n",
            "Epoch 30/1000\n",
            "47/47 [==============================] - 17s 366ms/step - loss: 0.0922 - accuracy: 0.9663 - val_loss: 1.4091 - val_accuracy: 0.6769\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 17s 366ms/step - loss: 0.0897 - accuracy: 0.9663 - val_loss: 1.1739 - val_accuracy: 0.6792\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 17s 363ms/step - loss: 0.0841 - accuracy: 0.9676 - val_loss: 1.2504 - val_accuracy: 0.7007\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 17s 367ms/step - loss: 0.0845 - accuracy: 0.9672 - val_loss: 1.8152 - val_accuracy: 0.6370\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 17s 365ms/step - loss: 0.0859 - accuracy: 0.9675 - val_loss: 1.2008 - val_accuracy: 0.7030\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 17s 369ms/step - loss: 0.0780 - accuracy: 0.9714 - val_loss: 1.6542 - val_accuracy: 0.6431\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 17s 365ms/step - loss: 0.0750 - accuracy: 0.9714 - val_loss: 1.4003 - val_accuracy: 0.6899\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 17s 364ms/step - loss: 0.0796 - accuracy: 0.9711 - val_loss: 1.9953 - val_accuracy: 0.6101\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 17s 365ms/step - loss: 0.0760 - accuracy: 0.9705 - val_loss: 1.2738 - val_accuracy: 0.7061\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 17s 367ms/step - loss: 0.0705 - accuracy: 0.9739 - val_loss: 1.0036 - val_accuracy: 0.6984\n",
            "Epoch 40/1000\n",
            "47/47 [==============================] - 17s 372ms/step - loss: 0.0736 - accuracy: 0.9722 - val_loss: 1.4728 - val_accuracy: 0.6830\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 17s 366ms/step - loss: 0.0729 - accuracy: 0.9731 - val_loss: 1.5079 - val_accuracy: 0.6907\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 18s 377ms/step - loss: 0.0690 - accuracy: 0.9732 - val_loss: 1.2253 - val_accuracy: 0.7038\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 18s 372ms/step - loss: 0.0636 - accuracy: 0.9761 - val_loss: 1.1590 - val_accuracy: 0.6830\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 17s 369ms/step - loss: 0.0679 - accuracy: 0.9747 - val_loss: 0.9674 - val_accuracy: 0.6976\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 17s 372ms/step - loss: 0.0670 - accuracy: 0.9752 - val_loss: 1.2834 - val_accuracy: 0.6969\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 17s 367ms/step - loss: 0.0627 - accuracy: 0.9775 - val_loss: 0.8215 - val_accuracy: 0.7030\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 18s 372ms/step - loss: 0.0666 - accuracy: 0.9754 - val_loss: 1.9893 - val_accuracy: 0.6447\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 17s 368ms/step - loss: 0.0630 - accuracy: 0.9769 - val_loss: 1.5008 - val_accuracy: 0.6884\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 17s 365ms/step - loss: 0.0638 - accuracy: 0.9761 - val_loss: 1.4183 - val_accuracy: 0.6715\n",
            "Epoch 50/1000\n",
            "47/47 [==============================] - 17s 371ms/step - loss: 0.0572 - accuracy: 0.9787 - val_loss: 1.5618 - val_accuracy: 0.6631\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 17s 371ms/step - loss: 0.0550 - accuracy: 0.9792 - val_loss: 1.7356 - val_accuracy: 0.6846\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 17s 370ms/step - loss: 0.0584 - accuracy: 0.9783 - val_loss: 1.5318 - val_accuracy: 0.6807\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 17s 366ms/step - loss: 0.0555 - accuracy: 0.9796 - val_loss: 1.6347 - val_accuracy: 0.6846\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 17s 369ms/step - loss: 0.0553 - accuracy: 0.9797 - val_loss: 1.6249 - val_accuracy: 0.7053\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd9335e27c0>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fitting the model using the train data\n",
        "\n",
        "callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=50)\n",
        "mcCallback = ModelCheckpoint('./2LSTM_sci.h5', monitor='val_loss')\n",
        "model.fit([array(trainData[0]), array(trainData[1])], array(trainData[2]), batch_size=batchSize, epochs=epochs, callbacks=[callback, mcCallback], validation_data=[[array(validationData[0]), array(validationData[1])], array(validationData[2])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('../../Models/2LSTM_sci.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.models.load_model('../../Models/2LSTM_sci.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "C58V0Ch76Znb",
        "outputId": "7f877f90-f67f-4eaa-a940-70268bd9b5d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 1s 65ms/step - loss: 1.6429 - accuracy: 0.6965\n",
            "Loss =  1.6428693532943726\n",
            "Acc =  0.6964705586433411\n"
          ]
        }
      ],
      "source": [
        "# Evaluating accuracy on the trained model\n",
        "\n",
        "loss, acc = model.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
        "print('Loss = ', loss)\n",
        "print('Acc = ', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can thus observe the following results for the shared LSTM model, for the SciTail dataset:\n",
        "\n",
        "- Training Accuracy: 97.97%\n",
        "- Validation Accuracy: 70.53%\n",
        "- Test Accuracy: 69.64%\n",
        "\n",
        "We can see, based on the training and test data, that the model has slightly overfit on the data. This can be controlled to a certain degree by optimizing the patience value for early stopping and other hyperparameters. However, as we observe in the SICK dataset, the amount of data of the dataset isn't sufficient for the shared LSTM model, and it is thus suitable for larger datasets like SNLI."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of summationBowman.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
